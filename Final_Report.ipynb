{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tanzania Water Well Classification Project: Does it Need Repair?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **About**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The purpose of this notebook is to share a summary of the methodology used during this project. Here you can find the goals of the project, what factors were taken into account during the Exploratory Data Analysis(EDA) phase, examples from the model building process, our final model and evaluation, and considerations and decisions taken along the way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Poject Goals***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal of this project is to build a classification model to know if a waterpump is in need of repair. \n",
    "\n",
    "#### The problem this project is addressing is access to water by way of community waterpoints. Access to water is an important issue that has reverberations in the social and economic aspects of a society. The [World Health Organization writes](https://www.who.int/news-room/fact-sheets/detail/drinking-water#:~:text=Safe%20and%20readily%20available%20water,contribute%20greatly%20to%20poverty%20reduction.), “Safe and readily available water is important for public health, whether it is used for drinking, domestic use, food production or recreational purposes. Improved water supply and sanitation, and better management of water resources, can boost countries’ economic growth and can contribute greatly to poverty reduction.” \n",
    "\n",
    "#### The water pumps we are looking at in our modeling are meant to provide potable water. If these water pumps fail, that community’s availability of drinking water is impacted. Reduced availability of working water pumps means an increased use of the functional ones, which could mean a reduction in that water pumps lifespan before it needs repairs. Being able to know which waterpoints need maintenance can help that community have a minimal interruption of service.\n",
    "\n",
    "#### This model will be used by the Tanzanian Ministry of Water to assess which water pumps need to be repaired. This model helps the goals of the Tanzanian Ministry of Water to provide access to water to its citizens. By knowing which water pumps need repairs, the Ministry of Water can implement better maintenance strategies. Moreover, this model and implementation of outcomes can help towards [The Tanzanian Development Vision 2025](https://mof.go.tz/mofdocs/overarch/vision2025.htm) to have universal access to safe water by the year 2025."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***The Data***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The **data comes from** [Taarifa](http://taarifa.org/) who sources it from the [Tanzanian Ministry of Water](https://www.maji.go.tz/). The datasets were downloaded from [Driven Data’s “Pump it Up: Data Mining the Water Table”](https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/) competition.\n",
    "\n",
    "#### Our target is to classify the water pumps into one of three possible categories:\n",
    "1. `functional` - the waterpoint is operational and there are no repairs needed\n",
    "2. `functional needs repair` - the waterpoint is operational, but needs repairs\n",
    "3. `non functional` - the waterpoint is not operational\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory Data Analysis (EDA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A few of the questions we explored before going into our modeling were:\n",
    "    - What features are available to us? Do we need all of them?\n",
    "    - What format are our features in? \n",
    "    - Are there missing values in our datasets? how will we account for those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAUL'S CODE AND COMMENTS\n",
    "ADD CODE HERE\n",
    "-column names...some examples of the on categorical, looked for reduntant variables in the catergorical data\n",
    "- dtypes-- write out thinking \n",
    "- checked for missing values, discuss how will take care of them when building model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "src_path = pathlib.Path().absolute()/\"src\"\n",
    "sys.path.append(str(src_path))\n",
    "import data_functions\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data to DataFrames\n",
    "data_path = pathlib.Path().absolute()/\"data\"\n",
    "x_train, x_test, y_train = data_functions.get_dataframes(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we want to take a look at our missing values. If we have any we want to know where they are, how many there are, and what problems we may have if we drop them. We also want to divide our data into numerical and categorical data. We will look at these different variable types respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount_tsh           0\n",
      "gps_height           0\n",
      "longitude            0\n",
      "latitude             0\n",
      "num_private          0\n",
      "region_code          0\n",
      "district_code        0\n",
      "population           0\n",
      "construction_year    0\n",
      "dtype: int64\n",
      "date_recorded                0\n",
      "funder                    3635\n",
      "installer                 3655\n",
      "wpt_name                     0\n",
      "basin                        0\n",
      "subvillage                 371\n",
      "region                       0\n",
      "lga                          0\n",
      "ward                         0\n",
      "public_meeting            3334\n",
      "recorded_by                  0\n",
      "scheme_management         3877\n",
      "scheme_name              28166\n",
      "permit                    3056\n",
      "extraction_type              0\n",
      "extraction_type_group        0\n",
      "extraction_type_class        0\n",
      "management                   0\n",
      "management_group             0\n",
      "payment                      0\n",
      "payment_type                 0\n",
      "water_quality                0\n",
      "quality_group                0\n",
      "quantity                     0\n",
      "quantity_group               0\n",
      "source                       0\n",
      "source_type                  0\n",
      "source_class                 0\n",
      "waterpoint_type              0\n",
      "waterpoint_type_group        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_nums_train=x_train.select_dtypes(exclude=\"object\")\n",
    "x_cat_train= x_train.select_dtypes(include=\"object\")\n",
    "print(x_nums_train.isna().sum())\n",
    "print(x_cat_train.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of our numerical data contains any missing values, which is a great start. However our categorical data is missing a lot of information, which is going to be problematic when we go take our data to modeling. Fortunately for us when exploring our categorical data, none of the columns that are missing data, are any columns that we want to keep around. For example scheme name doesn't really provide our model any meaningful information when it comes to identifying what condition the wells are in. We also went through our categorical data and looked at value counts. Knowing that in the future we would need to OneHotEncode our categorical data we decided to only proceed with columns that would provide us with meaningful data, and also was not redudant. Another win for us is that in our final feature selection we ended up only with columns whose unique value counts were less than 10. Our final column feature dataframe after joining numerical variables back with categorical data had 207 columns. That's alot of columns, but still managable when fitting models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "K                                     682\n",
       "None                                  644\n",
       "Borehole                              546\n",
       "Chalinze wate                         405\n",
       "M                                     400\n",
       "                                     ... \n",
       "Windmili system                         1\n",
       "Pwani water supply                      1\n",
       "Makonde Water supply                    1\n",
       "Flood water sustainability project      1\n",
       "Kifunikoni water supply                 1\n",
       "Name: scheme_name, Length: 2696, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat_train.scheme_name.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "groundwater    45794\n",
       "surface        13328\n",
       "unknown          278\n",
       "Name: source_class, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_cat_train.source_class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First Simple Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import custom functions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can begin modeling we first have to clean the data by dropping any features that aren't relevant to the modeling process as well as scalling numeric features and one hot encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data to DataFrames\n",
    "data_path = pathlib.Path().absolute()/\"data\"\n",
    "x_train, x_test, y_train = data_functions.get_dataframes(data_path)\n",
    "\n",
    "# Perform data preprocessing to improve modeling results\n",
    "x_train_pp, y_train_pp = data_functions.data_preprocessing(x_train, y_train)\n",
    "\n",
    "# Perform a train test split for validation after modeling\n",
    "x_tr_split, x_te_split, y_tr_split, y_te_split = train_test_split(x_train_pp, y_train_pp, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Initial Model Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAUL adds code for the confustion matrix with the 3 classifications. KNN and RF?\n",
    "ADD CODE for example models with the 3 classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Random Forest Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-dabe9097a8d6>:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf.fit(x_tr_split, y_tr_split)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RandomForestClassifier' object has no attribute 'pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-dabe9097a8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_tr_split\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_pred_rf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_te_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomForestClassifier' object has no attribute 'pred'"
     ]
    }
   ],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf.fit(x_tr_split, y_tr_split)\n",
    "y_pred_rf= rf.pred(x_te_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_2=RandomForestClassifier(max_depth=25, max_features='sqrt', min_samples_split=7, n_estimators=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***KNeighbor Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn3_2 = KNeighborsClassifier(n_neighbors = 15)\n",
    "kn3_2.fit(X_tr_sm,y_tr_sm)\n",
    "y_pred_kn3_2=kn3_2.predict(x_val_test)\n",
    "print(classification_report(y_val_test, y_pred_kn3_2))\n",
    "plot_confusion_matrix(kn3_2, x_val_test,y_val_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example of hypertuning, did not improve recall, only accuracy as may be expected\n",
    "kn3_5 = KNeighborsClassifier(n_neighbors = 3, weights = 'distance', p = 1, algorithm = 'brute')\n",
    "kn3_5.fit(X_tr_sm,y_tr_sm)\n",
    "y_pred_kn3_5=kn3_5.predict(x_val_test)\n",
    "print(classification_report(y_val_test, y_pred_kn3_5))\n",
    "plot_confusion_matrix(kn3_5, x_val_test,y_val_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tesing various models and up-sampling techniques such as SMOTE, we were unable to reliably classify our minority class 'functional, needs repair'. Because of this we decided that the most important thing is to classify any well that needed repair and that combining 'functional, needs repair' and 'non-functional' would be an effective way to eliminate our minority class while still answering the fundamental question of which wells need repair and which wells are completely functional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that converts the 'functional' class to 1 and every other class to 0\n",
    "bin_y = lambda x: 1 if x == 'functional' else 0\n",
    "# apply the function to y_train and y_test\n",
    "y_tr_split = pd.DataFrame(y_tr_split['status_group'].apply(bin_y), index = y_tr_split.index, columns = y_tr_split.columns)\n",
    "y_te_split = pd.DataFrame(y_te_split['status_group'].apply(bin_y), index = y_te_split.index, columns = y_te_split.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Random Forest Classifier***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PAUL Add 1 or two examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***KNeighbor Classifier***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1. This was model iteration number 2 begining to use hyperparameters to tune\n",
    "kn_2 = KNeighborsClassifier(n_neighbors = 15)\n",
    "kn_2.fit(x_val,y_tr_final)\n",
    "y_pred_kn_2=kn_2.predict(x_val_test)\n",
    "print(classification_report(y_te_final, y_pred_kn_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2. This was model iteration number 8 using hyperparameter tuning\n",
    "kn_8 = KNeighborsClassifier(n_neighbors = 2, weights = 'distance', p = 1, algorithm = 'brute')\n",
    "kn_8.fit(x_val,y_tr_final)\n",
    "y_pred_kn_8=kn_8.predict(x_val_test)\n",
    "print(classification_report(y_te_final, y_pred_kn_8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Logistic Classification***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypertuned for regularization strength (C) and class_weight. First model turned out to be the best for this type of classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_1= LogisticRegression(max_iter= 2000)\n",
    "lr_1.fit(x_val,y_tr_final)\n",
    "y_pred_lr_1=lr_1.predict(x_val_test)\n",
    "print(classification_report(y_te_final, y_pred_lr_1))\n",
    "plot_confusion_matrix(lr_1, x_val_test,y_te_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Final Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We came to our final model by grid searching with a random forest classifier. The best parameters found in the grid search were: max_features = 75, min_samples_leaf = 3, n_estimators = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a RandomForestClassifier object with our best parameters\n",
    "final_model = RandomForestClassifier(max_features = 75, min_samples_leaf = 3, n_estimators = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_features=75, min_samples_leaf=3, n_estimators=200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting our classifier on the training split\n",
    "final_model.fit(x_tr_split, y_tr_split['status_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8098989898989899"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scoring our model's accuracy on the testing split\n",
    "final_model.score(x_te_split, y_te_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
